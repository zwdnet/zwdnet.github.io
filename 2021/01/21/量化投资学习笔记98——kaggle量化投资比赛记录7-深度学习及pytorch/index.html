<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">


<meta name="google-site-verification" content="E9deYnivN5MuHMuIfiMZZfS0alv-d_0UjcwjBL79lGU" />



<meta name="baidu-site-verification" content="iHYWJxscwD" />










<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="true" />








  <meta name="baidu-site-verification" content="true" />







  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="学习笔记,量化投资,kaggle竞赛,深度学习,pytorch," />





  <link rel="alternate" href="/atom.xml" title="赵瑜敏的口腔医学专业学习博客" type="application/atom+xml" />






<meta name="description" content="本文主要参考kaggle上的两篇文章：Deep Learning Tutorial for BeginnersPytorch Tutorial for Deep Learning Lovers不是全文翻译，算是我的学习笔记吧。先看Deep Learning Tutorial for Beginners。深度学习，是一种直接从数据中学习特征的机器学习技术。（Deep learning: One of">
<meta property="og:type" content="article">
<meta property="og:title" content="量化投资学习笔记98——kaggle量化投资比赛记录7-深度学习及pytorch">
<meta property="og:url" content="https://zwdnet.github.io/2021/01/21/%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B098%E2%80%94%E2%80%94kaggle%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84%E6%AF%94%E8%B5%9B%E8%AE%B0%E5%BD%957-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8F%8Apytorch/index.html">
<meta property="og:site_name" content="赵瑜敏的口腔医学专业学习博客">
<meta property="og:description" content="本文主要参考kaggle上的两篇文章：Deep Learning Tutorial for BeginnersPytorch Tutorial for Deep Learning Lovers不是全文翻译，算是我的学习笔记吧。先看Deep Learning Tutorial for Beginners。深度学习，是一种直接从数据中学习特征的机器学习技术。（Deep learning: One of">
<meta property="og:locale">
<meta property="og:image" content="https://zymblog-1258069789.cos.ap-chengdu.myqcloud.com/blog0234-dp/data.png">
<meta property="og:image" content="https://zymblog-1258069789.cos.ap-chengdu.myqcloud.com/blog0234-dp/02.jpg">
<meta property="og:image" content="https://zymblog-1258069789.cos.ap-chengdu.myqcloud.com/blog0234-dp/03.png">
<meta property="og:image" content="https://zymblog-1258069789.cos.ap-chengdu.myqcloud.com/blog0234-dp/04.png">
<meta property="og:image" content="https://zymblog-1258069789.cos.ap-chengdu.myqcloud.com/blog0234-dp/05.jpg">
<meta property="og:image" content="https://zymblog-1258069789.cos.ap-chengdu.myqcloud.com/blog0234-dp/06.png">
<meta property="og:image" content="https://zymblog-1258069789.cos.ap-chengdu.myqcloud.com/blog0234-dp/07.png">
<meta property="og:image" content="https://zymblog-1258069789.cos.ap-chengdu.myqcloud.com/blog0234-dp/08.png">
<meta property="og:image" content="https://zymblog-1258069789.cos.ap-chengdu.myqcloud.com/blog0234-dp/09.png">
<meta property="og:image" content="https://zymblog-1258069789.cos.ap-chengdu.myqcloud.com/blog0234-dp/10.png">
<meta property="og:image" content="https://zymblog-1258069789.cos.ap-chengdu.myqcloud.com/blog0234-dp/11.png">
<meta property="og:image" content="https://zymblog-1258069789.cos.ap-chengdu.myqcloud.com/blog0234-dp/12.png">
<meta property="og:image" content="https://zymblog-1258069789.cos.ap-chengdu.myqcloud.com/other/wx.jpg">
<meta property="article:published_time" content="2021-01-20T22:41:04.000Z">
<meta property="article:modified_time" content="2021-09-24T16:10:59.000Z">
<meta property="article:author" content="赵瑜敏">
<meta property="article:tag" content="学习笔记">
<meta property="article:tag" content="量化投资">
<meta property="article:tag" content="kaggle竞赛">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zymblog-1258069789.cos.ap-chengdu.myqcloud.com/blog0234-dp/data.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://zwdnet.github.io/2021/01/21/量化投资学习笔记98——kaggle量化投资比赛记录7-深度学习及pytorch/"/>





  <title>量化投资学习笔记98——kaggle量化投资比赛记录7-深度学习及pytorch | 赵瑜敏的口腔医学专业学习博客</title>
  








<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">赵瑜敏的口腔医学专业学习博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zwdnet.github.io/2021/01/21/%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B098%E2%80%94%E2%80%94kaggle%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84%E6%AF%94%E8%B5%9B%E8%AE%B0%E5%BD%957-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8F%8Apytorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://zymblog-1258069789.cos.ap-chengdu.myqcloud.com/other/tx.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="赵瑜敏的口腔医学专业学习博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">量化投资学习笔记98——kaggle量化投资比赛记录7-深度学习及pytorch</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-01-21T06:41:04+08:00">
                2021-01-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84/" itemprop="url" rel="index">
                    <span itemprop="name">量化投资</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                  <span class="post-meta-divider">|</span>
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  6k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  27
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>本文主要参考kaggle上的两篇文章：<br><a target="_blank" rel="noopener" href="https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners">Deep Learning Tutorial for Beginners</a><br><a target="_blank" rel="noopener" href="https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers">Pytorch Tutorial for Deep Learning Lovers</a><br>不是全文翻译，算是我的学习笔记吧。<br>先看Deep Learning Tutorial for Beginners。<br>深度学习，是一种直接从数据中学习特征的机器学习技术。（Deep learning: One of the machine learning technique that learns features directly from data.）随着数据规模上升（如超过1百万个数据），传统机器学习技术不太适合，深度学习在准确率方面有更好的表现。深度学习应用在语音识别，图像分类，自然语言处理(nlp)或者推荐系统等方面。机器学习包括深度学习，在机器学习中，特征是人工标注的，而深度学习中特征是直接从数据中学习出来的。<br>实验数据是2062个手语数字图像，从0到9，一共10个不同的符号。0的序号从204到408,有205个。1的序号从822到1027,有206个。先只考虑0和1两个数字，因此每个分类有205个样本。尽管205个样本对深度学习来说太少了，但这是教程，就不管了。<br>先加载数据（从教程页面上下载，放到源代码目录。）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&quot;ignore&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loadData</span>():</span><br><span class="line">    x_1 = np.load(<span class="string">&quot;./X.npy&quot;</span>)</span><br><span class="line">    y_1 = np.load(<span class="string">&quot;./Y.npy&quot;</span>)</span><br><span class="line">    img_size = <span class="number">64</span></span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    plt.imshow(x_1[<span class="number">260</span>].reshape(img_size, img_size))</span><br><span class="line">    plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    plt.imshow(x_1[<span class="number">900</span>].reshape(img_size, img_size))</span><br><span class="line">    plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">    plt.savefig(<span class="string">&quot;./output/data.png&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://zymblog-1258069789.cos.ap-chengdu.myqcloud.com/blog0234-dp/data.png"><br>把数据连接起来，并创建标签。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把数据连接起来，并创建标签</span></span><br><span class="line">X = np.concatenate((x_1[<span class="number">204</span>:<span class="number">409</span>], x_1[<span class="number">822</span>:<span class="number">1027</span>]), axis = <span class="number">0</span>)</span><br><span class="line">z = np.zeros(<span class="number">205</span>)</span><br><span class="line">o = np.ones(<span class="number">205</span>)</span><br><span class="line">Y = np.concatenate((z, o), axis = <span class="number">0</span>).reshape(X.shape[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(X.shape)</span><br><span class="line"><span class="built_in">print</span>(Y.shape)</span><br></pre></td></tr></table></figure>

<p>X的大小是(410, 64, 64)，即410个图片，每个图片的大小是64×64<br>Y的大小是(410, 1)，即有410个标签。<br>现在将数据划分为训练集和测试集，其中训练集占85%。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = <span class="number">0.15</span>, random_state = <span class="number">42</span>)</span><br><span class="line">number_of_train = X_train.shape[<span class="number">0</span>]</span><br><span class="line">number_of_test = X_test.shape[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<p>将三维数据变换到二维(flatten)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将三维数据变换到二维</span></span><br><span class="line">X_train_flatten = X_train.reshape(number_of_train, X_train.shape[<span class="number">1</span>]*X_train.shape[<span class="number">2</span>])</span><br><span class="line">X_test_flatten = X_test.reshape(number_of_test, X_test.shape[<span class="number">1</span>]*X_test.shape[<span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X_train_flatten&quot;</span>, X_train_flatten.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X_test_flatten&quot;</span>, X_test_flatten.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train_flatten (<span class="number">348</span>, <span class="number">4096</span>)</span><br><span class="line">X_test_flatten (<span class="number">62</span>, <span class="number">4096</span>)</span><br></pre></td></tr></table></figure>

<p>将数据倒置</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x_train = X_train_flatten.T</span><br><span class="line">x_test = X_test_flatten.T</span><br><span class="line">y_train = Y_train.T</span><br><span class="line">y_test = Y_test.T</span><br><span class="line"><span class="built_in">print</span>(x_train.shape)</span><br><span class="line"><span class="built_in">print</span>(x_test.shape)</span><br><span class="line"><span class="built_in">print</span>(y_train.shape)</span><br><span class="line"><span class="built_in">print</span>(y_test.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">4096</span>, <span class="number">348</span>)</span><br><span class="line">(<span class="number">4096</span>, <span class="number">62</span>)</span><br><span class="line">(<span class="number">1</span>, <span class="number">348</span>)</span><br><span class="line">(<span class="number">1</span>, <span class="number">62</span>)</span><br></pre></td></tr></table></figure>

<p>数据准备好了，下面开始干活。<br>一想到二分类问题我们首先想到的是逻辑回归。实际上逻辑回归是一个非常简单的神经网络。神经网络和深度学习是一回事。<br>计算图(computation graph)的概念<br>数学表达式的可视化。这个我这只能看到图片的一部分，折腾半天没弄下来，大家到网站上看吧。<br>逻辑回归同样有计算图。参数是权重(weight)和偏差值(bias)。权重是每个点的系数，偏差值是截距。<br>z &#x3D; (w.t)x+b<br>另一个说法：z &#x3D; b+px1w1+px2w2+…+px4096*w4096<br>yhead &#x3D; sigmoid(z)<br>sigmoid使得z在[0,1]区间内。即一个概率值。<br>sigmoid函数的计算图<br><img src="https://zymblog-1258069789.cos.ap-chengdu.myqcloud.com/blog0234-dp/02.jpg"><br>为什么我们使用sigmoid函数？它返回一个概率性的结果，它是可微的，因此我们可以使用梯度下降算法。<br>下面我们初始化参数。<br>输入的数据是有4096个点的图像，每个点都有其自己的权重值。第一步是将每个点乘以其自己的权重值。初始权重值的设置有不同的方法，这里设置为0.01。偏差值为0。<br>下面是代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_params</span>(<span class="params">dimension</span>):</span><br><span class="line">    w = np.full((dimension, <span class="number">1</span>), <span class="number">0.01</span>)</span><br><span class="line">    b = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">return</span> w, b</span><br></pre></td></tr></table></figure>
<p>下面看前向传播过程：从输入点数据到成本的所有步骤称为前向传播(Forward Propagation)。<br>z &#x3D; (w.T)x + b，w和b都知道了（.T是转置），可以算出z。将z输入sigmoid函数得到返回的概率值yhat。然后计算损失&#x2F;误差函数(loss&#x2F;error)。所有损失值之和就是成本。<br>下面撸代码：<br>先定义sigmoid函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义sigmoid函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">    y_head = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</span><br><span class="line">    <span class="keyword">return</span> y_head</span><br></pre></td></tr></table></figure>
<p>然后计算损失函数，使得当模型预测正确时损失值很小，而当预测错误时损失值很大。<br>下面实现前向传播过程。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 前向传播过程</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fp</span>(<span class="params">w, b, x_train, y_train</span>):</span><br><span class="line">    z = np.dot(w.T, x_train) + b</span><br><span class="line">    y_head = sigmoid(z)</span><br><span class="line">    loss = -y_train*np.log(y_head) - (<span class="number">1</span>-y_train)*np.log(<span class="number">1</span>-y_head)</span><br><span class="line">    <span class="comment"># 平均成本</span></span><br><span class="line">    cost = (np.<span class="built_in">sum</span>(loss))/x_train.shape[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure>

<p>采用梯度下降的优化算法<br>我们需要降低成本。首先初始化权重和偏置值，这决定了初始成本。然后要更新权重和偏置值。这项技术称为梯度下降。<br>更新的方法，用老的参数值减去在该点的梯度，将该值作为参数的新的值。计算梯度的方法，是求损失函数在该点对该参数的偏导数。梯度同时确定了迭代的大小和方向。在迭代的时候，梯度要乘以一个学习率α。w’ &#x3D; w - α∂L&#x2F;∂w 学习率是需要权衡的，太小，学习得太慢，但不容易错过最低值。太大，学习得快，但容易错过最低值。学习率也被称为超参数(hyperparameter)，是需要选择和调参的。因此前向过程就是从参数到成本，反向过程就是从成本到参数，更新参数。怎么计算梯度及更新参数，就是数学内容了。直接上结果吧。<br><img src="https://zymblog-1258069789.cos.ap-chengdu.myqcloud.com/blog0234-dp/03.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 前后向传播过程</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fbp</span>(<span class="params">w, b, x_train, y_train</span>):</span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    z = np.dot(w.T, x_train) + b</span><br><span class="line">    y_head = sigmoid(z)</span><br><span class="line">    loss = -y_train*np.log(y_head) - (<span class="number">1</span>-y_train)*np.log(<span class="number">1</span>-y_head)</span><br><span class="line">    cost = (np.<span class="built_in">sum</span>(loss))/x_train.shape[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 后向传播过程</span></span><br><span class="line">    dw = (np.dot(x_train, ((y_head-y_train).T)))/x_train.shape[<span class="number">1</span>]</span><br><span class="line">    db = np.<span class="built_in">sum</span>(y_head-y_train)/x_train.shape[<span class="number">1</span>]</span><br><span class="line">    gradients = &#123;<span class="string">&quot;dw&quot;</span>:dw, <span class="string">&quot;db&quot;</span>:db&#125;</span><br><span class="line">    <span class="keyword">return</span> cost, gradients</span><br></pre></td></tr></table></figure>

<p>下面更新参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 更新参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">w, b, x_train, y_train, learning_rate, number_of_iteration</span>):</span><br><span class="line">    cost_list = []</span><br><span class="line">    cost_list2 = []</span><br><span class="line">    index = []</span><br><span class="line">    <span class="comment"># 更新(学习)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(number_of_iteration):</span><br><span class="line">        cost, gradients = fbp(w, b, x_train, y_train)</span><br><span class="line">        cost_list.append(cost)</span><br><span class="line">        <span class="comment"># 更新</span></span><br><span class="line">        w = w - learning_rate * gradients[<span class="string">&quot;dw&quot;</span>]</span><br><span class="line">        b = b - learning_rate * gradients[<span class="string">&quot;db&quot;</span>]</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            cost_list2 .append(cost)</span><br><span class="line">            index.append(i)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;第%i次迭代后的成本:%f&quot;</span> % (i, cost))</span><br><span class="line">        </span><br><span class="line">    parameters = &#123;<span class="string">&quot;weight&quot;</span>:w, <span class="string">&quot;bias&quot;</span>:b&#125;</span><br><span class="line">    plt.plot(index, cost_list2)</span><br><span class="line">    plt.savefig(<span class="string">&quot;./output/learning_curve.png&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> parameters, gradients, cost_list</span><br></pre></td></tr></table></figure>
<p><img src="https://zymblog-1258069789.cos.ap-chengdu.myqcloud.com/blog0234-dp/04.png"><br>下面进行预测。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进行预测</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">w, b, x_test</span>):</span><br><span class="line">    z = sigmoid(np.dot(w.T, x_test)+b)</span><br><span class="line">    Y_prediction = np.zeros((<span class="number">1</span>, x_test.shape[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(z.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">if</span> z[<span class="number">0</span>, i] &lt;= <span class="number">0.5</span>:</span><br><span class="line">            Y_prediction[<span class="number">0</span>, i] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            Y_prediction[<span class="number">0</span>, i] = <span class="number">1</span></span><br><span class="line">           </span><br><span class="line">    <span class="keyword">return</span> Y_prediction</span><br></pre></td></tr></table></figure>

<p>最后用测试数据进行预测。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">    <span class="comment"># 进行预测</span></span><br><span class="line">    y_pred_test = predict(parameters[<span class="string">&quot;weight&quot;</span>], parameters[<span class="string">&quot;bias&quot;</span>], x_test)</span><br><span class="line">    y_pred_train = predict(parameters[<span class="string">&quot;weight&quot;</span>], parameters[<span class="string">&quot;bias&quot;</span>], x_train)</span><br><span class="line">    <span class="comment"># 计算准确率</span></span><br><span class="line">    train_accuracy = <span class="number">100</span> - np.mean(np.<span class="built_in">abs</span>(y_pred_train - y_train))*<span class="number">100</span></span><br><span class="line">    test_accuracy = <span class="number">100</span> - np.mean(np.<span class="built_in">abs</span>(y_pred_test - y_test))*<span class="number">100</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;训练集预测准确率%f&quot;</span> % (train_accuracy))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;测试集预测准确率%f&quot;</span> % (test_accuracy))</span><br><span class="line">训练集的准确率为<span class="number">93.68</span>%，测试集的准确率为<span class="number">95.16</span>%。</span><br><span class="line">    <span class="comment"># 用sklearn进行</span></span><br><span class="line">    <span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line">    logreg = linear_model.LogisticRegression(random_state = <span class="number">42</span>, max_iter = <span class="number">150</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;sklearn算法&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;训练集预测准确率%f&quot;</span> % (logreg.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;测试集预测准确率%f&quot;</span> % (logreg.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)))</span><br></pre></td></tr></table></figure>
<p>准确率分别为100%和96.8%。</p>
<p>人工神经网络(Artificial Neural Network, ANN)<br>又称深度神经网络(deep neural network）或深度学习(deep learning)。最基础的人工神经网络为将逻辑回归进行至少两次。在逻辑回归中，只有输入层和输出层，而在神经网络中，有至少一个隐藏层在输入层和输出层之间。“深度(deep)”是隐藏层的层数很多，有多少是一个相对的概念，随着硬件的发展不断增加。“隐藏”的意思是它们不能直接看到输入的训练数据。<br>如下图，有一个隐藏层，这样的神经网络有2层，在计算层数的时候输入层被忽略。</p>
<p><img src="https://zymblog-1258069789.cos.ap-chengdu.myqcloud.com/blog0234-dp/05.jpg"><br>隐藏层有3个节点，数量的选择是随意的，没有理由。节点的数量就像学习率一样，是一个超参数。输入和输出层的情况和逻辑回归中一样。其中用到了tanh函数，用作激活函数，比sigmoid产生的输出更加集中。它还能增加非线性，使得模型学习得更好。隐藏层是输入层的输出，是输出层的输入。<br>下面就来具体研究2层神经网络。<br>层数和参数的初始化。<br>将权重初始化为0.01，偏差为0。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化参数和层数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_nn_parameters</span>(<span class="params">x_train, y_train</span>):</span><br><span class="line">    parameters = &#123;</span><br><span class="line">        <span class="string">&quot;weight1&quot;</span> : np.random.randn(<span class="number">3</span>, x_train.shape[<span class="number">0</span>])*<span class="number">0.1</span>,</span><br><span class="line">        <span class="string">&quot;bias1&quot;</span> : np.zeros((<span class="number">3</span>, <span class="number">1</span>)),</span><br><span class="line">        <span class="string">&quot;weight2&quot;</span> : np.random.randn(y_train.shape[<span class="number">0</span>], <span class="number">3</span>)*<span class="number">0.1</span>,</span><br><span class="line">        <span class="string">&quot;bias2&quot;</span> : np.zeros((y_train.shape[<span class="number">0</span>], <span class="number">1</span>))</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<p>前向传播过程与逻辑回归基本一样，唯一的不同采用tanh函数，进行两次。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 神经网络前向传播过程</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fp_NN</span>(<span class="params">x_train, parameters</span>):</span><br><span class="line">    Z1 = np.dot(parameters[<span class="string">&quot;weight1&quot;</span>], x_train) + parameters[<span class="string">&quot;bias1&quot;</span>]</span><br><span class="line">    A1 = np.tanh(Z1)</span><br><span class="line">    Z2 = np.dot(parameters[<span class="string">&quot;weight2&quot;</span>], A1) + parameters[<span class="string">&quot;bias2&quot;</span>]</span><br><span class="line">    A2 = sigmoid(Z2)</span><br><span class="line">   </span><br><span class="line">    cache = &#123;</span><br><span class="line">        <span class="string">&quot;Z1&quot;</span> : Z1,</span><br><span class="line">        <span class="string">&quot;A1&quot;</span> : A1,</span><br><span class="line">        <span class="string">&quot;Z2&quot;</span> : Z2,</span><br><span class="line">        <span class="string">&quot;A2&quot;</span> : A2</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure>
<p>损失函数跟逻辑回归一样，用交叉熵函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 神经网络的损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cost_NN</span>(<span class="params">A2, Y, parameters</span>):</span><br><span class="line">    logprobs = np.multiply(np.log(A2), Y)</span><br><span class="line">    cost = -np.<span class="built_in">sum</span>(logprobs)/Y.shape[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure>

<p>后向传播过程，即意味着求导。要了解数学内容，去看其它材料吧。逻辑跟逻辑回归是一样的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 神经网络后向传播过程</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bp_NN</span>(<span class="params">parameters, cache, X, Y</span>):</span><br><span class="line">    dZ2 = cache[<span class="string">&quot;A2&quot;</span>] - Y</span><br><span class="line">    dW2 = np.dot(dZ2, cache[<span class="string">&quot;A1&quot;</span>].T)/X.shape[<span class="number">1</span>]</span><br><span class="line">    db2 = np.<span class="built_in">sum</span>(dZ2, axis = <span class="number">1</span>, keepdims = <span class="literal">True</span>)/X.shape[<span class="number">1</span>]</span><br><span class="line">    dZ1 = np.dot(parameters[<span class="string">&quot;weight2&quot;</span>].T, dZ2)*(<span class="number">1</span>-np.power(cache[<span class="string">&quot;A1&quot;</span>], <span class="number">2</span>))</span><br><span class="line">    dW1 = np.dot(dZ1, X.T)/X.shape[<span class="number">1</span>]</span><br><span class="line">    db1 = np.<span class="built_in">sum</span>(dZ1, axis = <span class="number">1</span>, keepdims = <span class="literal">True</span>)/X.shape[<span class="number">1</span>]</span><br><span class="line">    grads = &#123;</span><br><span class="line">        <span class="string">&quot;dweight1&quot;</span> : dW1,</span><br><span class="line">        <span class="string">&quot;dbias1&quot;</span> : db1,</span><br><span class="line">        <span class="string">&quot;dweight2&quot;</span> : dW2,</span><br><span class="line">        <span class="string">&quot;dbias2&quot;</span> : db2</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure>

<p>更新参数，跟逻辑回归里一样的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 更新神经网络参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update_NN</span>(<span class="params">parameters, grads, learning_rate = <span class="number">0.01</span></span>):</span><br><span class="line">    parameters = &#123;</span><br><span class="line">        <span class="string">&quot;weight1&quot;</span> : parameters[<span class="string">&quot;weight1&quot;</span>] - learning_rate*grads[<span class="string">&quot;dweight1&quot;</span>],</span><br><span class="line">        <span class="string">&quot;bias1&quot;</span> : parameters[<span class="string">&quot;bias1&quot;</span>] - learning_rate*grads[<span class="string">&quot;dbias1&quot;</span>],</span><br><span class="line">        <span class="string">&quot;weight2&quot;</span> : parameters[<span class="string">&quot;weight2&quot;</span>] - learning_rate*grads[<span class="string">&quot;dweight2&quot;</span>],</span><br><span class="line">        <span class="string">&quot;bias2&quot;</span> : parameters[<span class="string">&quot;bias2&quot;</span>] - learning_rate*grads[<span class="string">&quot;dbias2&quot;</span>]</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>

<p>进行预测</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进行预测</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict_NN</span>(<span class="params">parameters, x_test</span>):</span><br><span class="line">    A2, cache = fp_NN(x_test, parameters)</span><br><span class="line">    Y_pred = np.zeros((<span class="number">1</span>, x_test.shape[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(A2.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">if</span> A2[<span class="number">0</span>, i] &lt;= <span class="number">0.5</span>:</span><br><span class="line">            Y_pred[<span class="number">0</span>, i] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            Y_pred[<span class="number">0</span>, i] = <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> Y_pred</span><br></pre></td></tr></table></figure>

<p>最后，建立神经网络</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span 